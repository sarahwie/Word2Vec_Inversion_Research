{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score    \n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import array\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "sys.path.append(os.path.expanduser('~/code/eol_hsrl_python'))\n",
    "os.environ['PYTHONHASHSEED']='10'\n",
    "#is this working?? no idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "df = pd.read_csv( 'labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0,  delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['id', 'boolean_label', 'abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Algorithm- only building basemodel vocab and training on 200 reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[22190 23112 18321 23640  5201  3919  2683  6224  3697  3396  7842 23548\n",
      "  8923  2552 18627  7834 10064  5625 10267 19770 18399 11077  3519 16556\n",
      "  6482]\n",
      "<type 'numpy.ndarray'>\n",
      "(200, 3)\n",
      "(100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /nlp-vm/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training w2v models\n",
      "0:04:19\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#get indices, 200 for training and 100 for testing randomly subsetted\n",
    "indices = random.sample(xrange(25000),300)\n",
    "indices = np.asarray(indices)\n",
    "print len(indices)\n",
    "print indices[0:25]\n",
    "print type(indices)\n",
    "#split indices into two sets\n",
    "train_index = indices[0:200]\n",
    "test_index = indices[200:300]\n",
    "\n",
    "#we'll build the basemodel on the entire vocabulary, but only train on about 200 reviews\n",
    "\n",
    "#use the indexes to subset the df pandas dataframe and get the associated rows\n",
    "train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "print train1.shape\n",
    "print test1.shape\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "for j in range(len(train1)):\n",
    "    review = train1['abstract'].iloc[j]\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Building and training w2v models\"\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    sentences=None,\n",
    "    size=50,\n",
    "    window=5,\n",
    "    workers=1, # don't use cores- use single worker thread for fully deterministically-reproducible run\n",
    "#             workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=1, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0,\n",
    "    alpha=0.10# we only have scoring for the hierarchical softmax setup\n",
    "    #left min_alpha as preset\n",
    "    )\n",
    "basemodel.build_vocab(sentences) \n",
    "\n",
    "# ****** Split the training set into clean sentences\n",
    "#\n",
    "sentences_pos = []  # Initialize an empty list of sentences\n",
    "sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "#here change to include all journal name labels of positive and negative\n",
    "inxs_pos = np.where(train1['boolean_label'] == 1)[0].tolist()\n",
    "inxs_neg = np.where(train1['boolean_label'] == 0)[0].tolist()\n",
    "\n",
    "for inx in inxs_pos:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "for inx in inxs_neg:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "#train models\n",
    "models = [deepcopy(basemodel) for y in range(2)]\n",
    "models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### see how fixed weights change on a single review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing test sentences\n"
     ]
    }
   ],
   "source": [
    "print \"Parsing test sentences\" #into positive and negative groups so know true label\n",
    "# read in the test set\n",
    "#NEW: now we are actually using the training set\n",
    "#and we only need a handful of reviews- 10 of each sent\n",
    "\n",
    "sentences_pos_test = []  # Initialize an empty list of sentences\n",
    "sentences_neg_test = []  # Initialize an empty list of sentences\n",
    "    \n",
    "inxs_pos_test = np.where(test1['boolean_label'] == 1)[0].tolist()\n",
    "inxs_neg_test = np.where(test1['boolean_label'] == 0)[0].tolist()\n",
    "\n",
    "\n",
    "for inx in inxs_pos_test[0:10]:\n",
    "    review = test1[\"abstract\"].iloc[inx]\n",
    "    sentences_pos_test += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    #print inx\n",
    "\n",
    "for inx in inxs_neg_test[0:10]:\n",
    "    review = test1[\"abstract\"].iloc[inx]\n",
    "    sentences_neg_test += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    #print inx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save matrices in new variables to load each time, as well as the negative and positive trained models\n",
    "neg1 = models[0].syn0\n",
    "neg2 = models[0].syn1\n",
    "modNeg = models[0]\n",
    "modPos = models[1]\n",
    "pos1 = models[1].syn0\n",
    "pos2 = models[1].syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'am', u'amazed', u'that', u'movies', u'like', u'this', u'can', u'still', u'be', u'made']\n"
     ]
    }
   ],
   "source": [
    "print sentences_pos_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrain model using a test sentence- \n",
    "modPos.train(sentences_pos_test[0], total_examples=1 )\n",
    "modNeg.train(sentences_pos_test[0], total_examples=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at weight matrices again....how much have they changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39795, 50)\n",
      "(39795, 50)\n",
      "(39795, 50)\n",
      "(39795, 50)\n"
     ]
    }
   ],
   "source": [
    "#look at dims\n",
    "print neg1.shape\n",
    "print neg2.shape\n",
    "print pos1.shape\n",
    "print pos2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: test that the original weights are actually being stored and not also updated. Then, try for every word in the sentence by recoding the below script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all = []\n",
    "#for sent in sentences_pos_test:\n",
    "#make copies of models\n",
    "#    copymodPos = modPos\n",
    "#    copymodNeg = modNeg\n",
    "a=[]\n",
    "#retrain model using a test sentence\n",
    "#    copymodPos.train(sent, total_examples=1 )\n",
    "#    copymodNeg.train(sent, total_examples=1 )\n",
    "#for each word....replace the 0's \n",
    "\n",
    "#NOTE: Changed FROM COPYMODNEG TO MODNEG AND SAME FOR MODPOS\n",
    "for i in range(models[0].syn0.shape[0]):\n",
    "    a.append(np.subtract(neg1[i], modNeg.syn0[i]))\n",
    "    a.append(np.subtract(neg2[i], modNeg.syn1[i]))\n",
    "    a.append(np.subtract(pos1[i], modPos.syn0[i]))\n",
    "    a.append(np.subtract(pos2[i], modPos.syn1[i]))\n",
    "\n",
    "    if np.any(np.not_equal(neg1[i],modNeg.syn0[i])):\n",
    "        print \"1\"\n",
    "        print i\n",
    "        print neg1[i]\n",
    "        print sent\n",
    "    elif np.any(np.not_equal(neg2[i],modNeg.syn1[i])):\n",
    "        print \"2\"\n",
    "        print i\n",
    "        print neg2[i]\n",
    "        print sent\n",
    "    elif np.any(np.not_equal(pos1[i],modPos.syn0[i])):\n",
    "        print \"3\"\n",
    "        print i\n",
    "        print pos1[i]\n",
    "        print sent\n",
    "    elif np.any(np.not_equal(pos2[i],modPos.syn1[i])):\n",
    "        print \"4\"\n",
    "        print i\n",
    "        print pos2[i]\n",
    "        print sent\n",
    "\n",
    "#then also append the score to the list*** TODO\n",
    "\n",
    "#all.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#repeat for sentences in sentences_neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.72277287e-02   1.26543835e-01   9.93497670e-03  -5.40592745e-02\n",
      "  -2.65558302e-01  -9.68429521e-02   8.76064673e-02  -1.33147120e-01\n",
      "   4.74467762e-02   1.02694966e-01   2.79902313e-02  -5.03830649e-02\n",
      "  -1.29035428e-01   8.29034764e-03   4.33472246e-02   2.05678880e-01\n",
      "   1.59809381e-01   3.38250399e-02  -2.10265428e-01  -1.30878299e-01\n",
      "  -1.95322976e-01  -2.52045747e-02   1.68052465e-01  -4.24853526e-02\n",
      "   1.19723156e-01   4.54059616e-02   4.93345559e-02  -6.80889115e-02\n",
      "  -2.11434722e-01  -1.75505623e-01  -1.63846120e-01   8.19133222e-02\n",
      "   8.09772909e-02   6.10496514e-02   3.02647091e-02   5.76601438e-02\n",
      "   6.89781224e-03   2.18035445e-01   1.95809379e-01  -1.67943567e-01\n",
      "   5.93783334e-05   7.96362162e-02  -3.42507352e-04  -8.06212574e-02\n",
      "   1.97106436e-01  -4.97829355e-02  -9.23664309e-03   1.54408872e-01\n",
      "  -9.93900597e-02  -1.28648549e-01]\n",
      "[  6.72277287e-02   1.26543835e-01   9.93497670e-03  -5.40592745e-02\n",
      "  -2.65558302e-01  -9.68429521e-02   8.76064673e-02  -1.33147120e-01\n",
      "   4.74467762e-02   1.02694966e-01   2.79902313e-02  -5.03830649e-02\n",
      "  -1.29035428e-01   8.29034764e-03   4.33472246e-02   2.05678880e-01\n",
      "   1.59809381e-01   3.38250399e-02  -2.10265428e-01  -1.30878299e-01\n",
      "  -1.95322976e-01  -2.52045747e-02   1.68052465e-01  -4.24853526e-02\n",
      "   1.19723156e-01   4.54059616e-02   4.93345559e-02  -6.80889115e-02\n",
      "  -2.11434722e-01  -1.75505623e-01  -1.63846120e-01   8.19133222e-02\n",
      "   8.09772909e-02   6.10496514e-02   3.02647091e-02   5.76601438e-02\n",
      "   6.89781224e-03   2.18035445e-01   1.95809379e-01  -1.67943567e-01\n",
      "   5.93783334e-05   7.96362162e-02  -3.42507352e-04  -8.06212574e-02\n",
      "   1.97106436e-01  -4.97829355e-02  -9.23664309e-03   1.54408872e-01\n",
      "  -9.93900597e-02  -1.28648549e-01]\n"
     ]
    }
   ],
   "source": [
    "print modPos[\"this\"]\n",
    "print models[1][\"this\"]\n",
    "#still identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.12128623e-01  -5.90351527e-04   1.90263346e-01  -2.04363316e-01\n",
      "  -1.25054032e-01   1.41420200e-01   2.07533948e-02  -5.16388237e-01\n",
      "  -2.71282345e-01  -2.74658352e-01   4.18581665e-01   6.85895383e-01\n",
      "   1.61994889e-01   1.41382843e-01   2.19165564e-01   6.37463620e-03\n",
      "   1.48713633e-01   4.50969517e-01   2.83410162e-01  -2.03582317e-01\n",
      "  -1.41550124e-01  -3.66136700e-01   1.05822659e+00  -7.81226233e-02\n",
      "   3.40196371e-01  -4.13311690e-01  -1.34569913e-01  -5.01110256e-02\n",
      "   5.74484766e-01   4.05226871e-02   1.15871266e-01  -9.73553479e-01\n",
      "  -1.10163584e-01  -3.38168532e-01   9.53039765e-01  -1.01441734e-01\n",
      "   4.13049966e-01   4.90189455e-02  -4.38492864e-01  -5.91962934e-02\n",
      "  -9.26226377e-01   4.83608961e-01  -7.15108156e-01   3.62209111e-01\n",
      "   6.68655038e-01   2.67568588e-01   3.60578388e-01   7.26784527e-01\n",
      "  -2.97102273e-01   4.04141605e-01  -4.31873351e-01  -1.42890409e-01\n",
      "   3.50398183e-01   3.67146939e-01   7.04841539e-02  -1.48633216e-02\n",
      "  -1.14153242e+00  -3.92584890e-01  -1.91272706e-01  -6.96127713e-01\n",
      "   3.76787841e-01   2.70745933e-01   5.74629068e-01  -1.07105839e+00\n",
      "  -3.46443474e-01  -9.17379200e-01  -2.87172258e-01   7.27824643e-02\n",
      "   8.88948962e-02  -5.95077932e-01  -8.39833796e-01   4.24457133e-01\n",
      "  -1.05815589e+00   1.29813388e-01   9.84567642e-01   3.42266828e-01\n",
      "  -6.08334020e-02   3.09696943e-01  -3.05879831e-01   9.24778223e-01\n",
      "  -3.03678304e-01  -1.45367965e-01   1.73873812e-01   2.79499054e-01\n",
      "   4.14949924e-01   6.28311932e-01  -2.95785129e-01  -9.13131416e-01\n",
      "  -3.65538187e-02  -8.89188349e-01  -4.66797948e-01   9.24781635e-02\n",
      "   7.95579851e-01   5.62470667e-02  -3.14890385e-01   1.65627122e-01\n",
      "   5.84659159e-01   8.44920099e-01   1.27778828e-01  -3.70635837e-01]\n",
      "[  1.12128623e-01  -5.90351527e-04   1.90263346e-01  -2.04363316e-01\n",
      "  -1.25054032e-01   1.41420200e-01   2.07533948e-02  -5.16388237e-01\n",
      "  -2.71282345e-01  -2.74658352e-01   4.18581665e-01   6.85895383e-01\n",
      "   1.61994889e-01   1.41382843e-01   2.19165564e-01   6.37463620e-03\n",
      "   1.48713633e-01   4.50969517e-01   2.83410162e-01  -2.03582317e-01\n",
      "  -1.41550124e-01  -3.66136700e-01   1.05822659e+00  -7.81226233e-02\n",
      "   3.40196371e-01  -4.13311690e-01  -1.34569913e-01  -5.01110256e-02\n",
      "   5.74484766e-01   4.05226871e-02   1.15871266e-01  -9.73553479e-01\n",
      "  -1.10163584e-01  -3.38168532e-01   9.53039765e-01  -1.01441734e-01\n",
      "   4.13049966e-01   4.90189455e-02  -4.38492864e-01  -5.91962934e-02\n",
      "  -9.26226377e-01   4.83608961e-01  -7.15108156e-01   3.62209111e-01\n",
      "   6.68655038e-01   2.67568588e-01   3.60578388e-01   7.26784527e-01\n",
      "  -2.97102273e-01   4.04141605e-01  -4.31873351e-01  -1.42890409e-01\n",
      "   3.50398183e-01   3.67146939e-01   7.04841539e-02  -1.48633216e-02\n",
      "  -1.14153242e+00  -3.92584890e-01  -1.91272706e-01  -6.96127713e-01\n",
      "   3.76787841e-01   2.70745933e-01   5.74629068e-01  -1.07105839e+00\n",
      "  -3.46443474e-01  -9.17379200e-01  -2.87172258e-01   7.27824643e-02\n",
      "   8.88948962e-02  -5.95077932e-01  -8.39833796e-01   4.24457133e-01\n",
      "  -1.05815589e+00   1.29813388e-01   9.84567642e-01   3.42266828e-01\n",
      "  -6.08334020e-02   3.09696943e-01  -3.05879831e-01   9.24778223e-01\n",
      "  -3.03678304e-01  -1.45367965e-01   1.73873812e-01   2.79499054e-01\n",
      "   4.14949924e-01   6.28311932e-01  -2.95785129e-01  -9.13131416e-01\n",
      "  -3.65538187e-02  -8.89188349e-01  -4.66797948e-01   9.24781635e-02\n",
      "   7.95579851e-01   5.62470667e-02  -3.14890385e-01   1.65627122e-01\n",
      "   5.84659159e-01   8.44920099e-01   1.27778828e-01  -3.70635837e-01]\n"
     ]
    }
   ],
   "source": [
    "print modNeg[\"this\"]\n",
    "print models[0][\"this\"]\n",
    "#still identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_to_words( review, remove_stopwords=False ):\n",
    "        # Function to convert a raw review to a string of words\n",
    "        # The input is a single string (a raw movie review), and \n",
    "        # the output is a single string (a preprocessed movie review)\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text() \n",
    "        #\n",
    "        # 2. Remove non-letters        \n",
    "        review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "        #\n",
    "        # 3. Convert to lower case, split into individual words\n",
    "        words = review_text.lower().split()                             \n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]   \n",
    "        #\n",
    "        # 6. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return( \" \".join( words ))   \n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
